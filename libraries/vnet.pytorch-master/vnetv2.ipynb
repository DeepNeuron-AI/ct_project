{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchbiomed.datasets as dset\n",
    "import torchbiomed.loss as bioloss\n",
    "import torchbiomed.utils as utils\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import shutil\n",
    "\n",
    "import setproctitle\n",
    "\n",
    "import vnet\n",
    "import make_graph\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "\n",
    "#nodule_masks = \"normalized_mask_5_0\"\n",
    "#lung_masks = \"normalized_seg_lungs_5_0\"\n",
    "#ct_images = \"normalized_CT_5_0\"\n",
    "#target_split = [1, 1, 1]\n",
    "\n",
    "\n",
    "nodule_masks = \"normalized_brightened_CT_2_5\"\n",
    "lung_masks = \"inferred_seg_lungs_2_5\"\n",
    "ct_images = \"luna16_ct_normalized\"\n",
    "ct_targets = nodule_masks\n",
    "target_split = [2, 2, 2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dice = False\n",
    "ngpu = 1\n",
    "n_epochs = 300\n",
    "start_epoch = 0\n",
    "               \n",
    "resume = ''\n",
    "evaluate = False\n",
    "inference_path_net = ''\n",
    "\n",
    "# 1e-8 works well for lung masks but seems to prevent\n",
    "# rapid learning for nodule masks\n",
    "weight_decay = 1e-8\n",
    "no_cuda = False\n",
    "save = True\n",
    "seed = 1\n",
    "opt ='adam'  # 'sgd', 'adam', 'rmsprop'\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv3d') != -1:\n",
    "        nn.init.kaiming_normal(m.weight)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "def datestr():\n",
    "    now = time.gmtime()\n",
    "    return '{}{:02}{:02}_{:02}{:02}'.format(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, path, prefix, filename='checkpoint.pth.tar'):\n",
    "    prefix_save = os.path.join(path, prefix)\n",
    "    name = prefix_save + '_' + filename\n",
    "    torch.save(state, name)\n",
    "    if is_best:\n",
    "        shutil.copyfile(name, prefix_save + '_model_best.pth.tar')\n",
    "\n",
    "\n",
    "def inference(loader, model, transforms):\n",
    "    src = inference_path_net\n",
    "    dst = save\n",
    "\n",
    "    model.eval()\n",
    "    nvols = reduce(operator.mul, target_split, 1)\n",
    "    # assume single GPU / batch size 1\n",
    "    for data in loader:\n",
    "        data, series, origin, spacing = data[0]\n",
    "        shape = data.size()\n",
    "        # convert names to batch tensor\n",
    "        if cuda:\n",
    "            data.pin_memory()\n",
    "            data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        _, output = output.max(1)\n",
    "        output = output.view(shape)\n",
    "        output = output.cpu()\n",
    "        # merge subvolumes and save\n",
    "        results = output.chunk(nvols)\n",
    "        results = map(lambda var : torch.squeeze(var.data).numpy().astype(np.int16), results)\n",
    "        volume = utils.merge_image([*results], target_split)\n",
    "        print(\"save {}\".format(series))\n",
    "        utils.save_updated_image(volume, os.path.join(dst, series + \".mhd\"), origin, spacing)\n",
    "\n",
    "# performing post-train inference:\n",
    "# train.py --resume <model checkpoint> --i <input directory (*.mhd)> --save <output directory>\n",
    "\n",
    "def noop(x):\n",
    "    return x\n",
    "\n",
    "def main():\n",
    "    \n",
    "    best_prec1 = 100.\n",
    "    cuda = not no_cuda and torch.cuda.is_available()\n",
    "    save = save or 'work/vnet.base.{}'.format(datestr())\n",
    "    nll = True\n",
    "    if dice:\n",
    "        nll = False\n",
    "    weight_decay = weight_decay\n",
    "    setproctitle.setproctitle(save)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    print(\"build vnet\")\n",
    "    model = vnet.VNet(elu=False, nll=nll)\n",
    "    batch_size = ngpu * batch_size\n",
    "    gpu_ids = range(ngpu)\n",
    "    model = nn.parallel.DataParallel(model, device_ids=gpu_ids)\n",
    "\n",
    "    if resume:\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(evaluate, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "    else:\n",
    "        model.apply(weights_init)\n",
    "\n",
    "    if nll:\n",
    "        train = train_nll\n",
    "        test = test_nll\n",
    "        class_balance = True\n",
    "    else:\n",
    "        train = train_dice\n",
    "        test = test_dice\n",
    "        class_balance = False\n",
    "\n",
    "    print('  + Number of params: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    if os.path.exists(save):\n",
    "        shutil.rmtree(save)\n",
    "    os.makedirs(save, exist_ok=True)\n",
    "\n",
    "    # LUNA16 dataset isotropically scaled to 2.5mm^3\n",
    "    # and then truncated or zero-padded to 160x128x160\n",
    "    normMu = [-642.794]\n",
    "    normSigma = [459.512]\n",
    "    normTransform = transforms.Normalize(normMu, normSigma)\n",
    "\n",
    "    trainTransform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "    testTransform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normTransform\n",
    "    ])\n",
    "    if ct_targets == nodule_masks:\n",
    "        masks = lung_masks\n",
    "    else:\n",
    "        masks = None\n",
    "\n",
    "    if inference_path_net != '':\n",
    "        if not resume:\n",
    "            print(\"resume must be set to do inference\")\n",
    "            exit(1)\n",
    "        kwargs = {'num_workers': 1} if cuda else {}\n",
    "        src = inference_path_net\n",
    "        dst = save\n",
    "        inference_batch_size = ngpu\n",
    "        root = os.path.dirname(src)\n",
    "        images = os.path.basename(src)\n",
    "        dataset = dset.LUNA16(root=root, images=images, transform=testTransform, split=target_split, mode=\"infer\")\n",
    "        loader = DataLoader(dataset, batch_size=inference_batch_size, shuffle=False, collate_fn=noop, **kwargs)\n",
    "        inference(loader, model, trainTransform)\n",
    "        return\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "    print(\"loading training set\")\n",
    "    trainSet = dset.LUNA16(root='luna16', images=ct_images, targets=ct_targets,\n",
    "                           mode=\"train\", transform=trainTransform, \n",
    "                           class_balance=class_balance, split=target_split, seed=seed, masks=masks)\n",
    "    trainLoader = DataLoader(trainSet, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    print(\"loading test set\")\n",
    "    testLoader = DataLoader(\n",
    "        dset.LUNA16(root='luna16', images=ct_images, targets=ct_targets,\n",
    "                    mode=\"test\", transform=testTransform, seed=seed, masks=masks, split=target_split),\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    target_mean = trainSet.target_mean()\n",
    "    bg_weight = target_mean / (1. + target_mean)\n",
    "    fg_weight = 1. - bg_weight\n",
    "    print(bg_weight)\n",
    "    class_weights = torch.FloatTensor([bg_weight, fg_weight])\n",
    "    if cuda:\n",
    "        class_weights = class_weights.cuda()\n",
    "\n",
    "    if opt == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=1e-1,\n",
    "                              momentum=0.99, weight_decay=weight_decay)\n",
    "    elif opt == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
    "    elif opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "    trainF = open(os.path.join(save, 'train.csv'), 'w')\n",
    "    testF = open(os.path.join(save, 'test.csv'), 'w')\n",
    "    err_best = 100.\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        adjust_opt(opt, optimizer, epoch)\n",
    "        train(epoch, model, trainLoader, optimizer, trainF, class_weights)\n",
    "        err = test(epoch, model, testLoader, optimizer, testF, class_weights)\n",
    "        is_best = False\n",
    "        if err < best_prec1:\n",
    "            is_best = True\n",
    "            best_prec1 = err\n",
    "        save_checkpoint({'epoch': epoch,\n",
    "                         'state_dict': model.state_dict(),\n",
    "                         'best_prec1': best_prec1},\n",
    "                        is_best, save, \"vnet\")\n",
    "        os.system('./plot.py {} {} &'.format(len(trainLoader), save))\n",
    "\n",
    "    trainF.close()\n",
    "    testF.close()\n",
    "\n",
    "\n",
    "def train_nll(epoch, model, trainLoader, optimizer, trainF, weights):\n",
    "    model.train()\n",
    "    nProcessed = 0\n",
    "    nTrain = len(trainLoader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(trainLoader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.view(target.numel())\n",
    "        loss = F.nll_loss(output, target, weight=weights)\n",
    "        dice_loss = bioloss.dice_error(output, target)\n",
    "        # make_graph.save('/tmp/t.dot', loss.creator); assert(False)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        nProcessed += len(data)\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        incorrect = pred.ne(target.data).cpu().sum()\n",
    "        err = 100.*incorrect/target.numel()\n",
    "        partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n",
    "        print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}\\tError: {:.3f}\\t Dice: {:.6f}'.format(\n",
    "            partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),\n",
    "            loss.data[0], err, dice_loss))\n",
    "\n",
    "        trainF.write('{},{},{}\\n'.format(partialEpoch, loss.data[0], err))\n",
    "        trainF.flush()\n",
    "\n",
    "\n",
    "def test_nll(epoch, model, testLoader, optimizer, testF, weights):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    dice_loss = 0\n",
    "    incorrect = 0\n",
    "    numel = 0\n",
    "    for data, target in testLoader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        target = target.view(target.numel())\n",
    "        numel += target.numel()\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, weight=weights).data[0]\n",
    "        dice_loss += bioloss.dice_error(output, target)\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        incorrect += pred.ne(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(testLoader)  # loss function already averages over batch size\n",
    "    dice_loss /= len(testLoader)\n",
    "    err = 100.*incorrect/numel\n",
    "    print('\\nTest set: Average loss: {:.4f}, Error: {}/{} ({:.3f}%) Dice: {:.6f}\\n'.format(\n",
    "        test_loss, incorrect, numel, err, dice_loss))\n",
    "\n",
    "    testF.write('{},{},{}\\n'.format(epoch, test_loss, err))\n",
    "    testF.flush()\n",
    "    return err\n",
    "\n",
    "\n",
    "def train_dice(epoch, model, trainLoader, optimizer, trainF, weights):\n",
    "    model.train()\n",
    "    nProcessed = 0\n",
    "    nTrain = len(trainLoader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(trainLoader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = bioloss.dice_loss(output, target)\n",
    "        # make_graph.save('/tmp/t.dot', loss.creator); assert(False)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        nProcessed += len(data)\n",
    "        err = 100.*(1. - loss.data[0])\n",
    "        partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n",
    "        print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.8f}\\tError: {:.8f}'.format(\n",
    "            partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),\n",
    "            loss.data[0], err))\n",
    "\n",
    "        trainF.write('{},{},{}\\n'.format(partialEpoch, loss.data[0], err))\n",
    "        trainF.flush()\n",
    "\n",
    "\n",
    "def test_dice(epoch, model, testLoader, optimizer, testF, weights):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    incorrect = 0\n",
    "    for data, target in testLoader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        loss = bioloss.dice_loss(output, target).data[0]\n",
    "        test_loss += loss\n",
    "        incorrect += (1. - loss)\n",
    "\n",
    "    test_loss /= len(testLoader)  # loss function already averages over batch size\n",
    "    nTotal = len(testLoader)\n",
    "    err = 100.*incorrect/nTotal\n",
    "    print('\\nTest set: Average Dice Coeff: {:.4f}, Error: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, incorrect, nTotal, err))\n",
    "\n",
    "    testF.write('{},{},{}\\n'.format(epoch, test_loss, err))\n",
    "    testF.flush()\n",
    "    return err\n",
    "\n",
    "\n",
    "def adjust_opt(optAlg, optimizer, epoch):\n",
    "    if optAlg == 'sgd':\n",
    "        if epoch < 150:\n",
    "            lr = 1e-1\n",
    "        elif epoch == 150:\n",
    "            lr = 1e-2\n",
    "        elif epoch == 225:\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}